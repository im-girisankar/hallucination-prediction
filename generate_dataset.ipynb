{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b0fa6cbf",
      "metadata": {
        "id": "b0fa6cbf"
      },
      "source": [
        "# Hallucination Dataset Generator ‚Äî 400 Queries\n",
        "\n",
        "**Purpose:** Create a clean, deduplicated, template-aware dataset of 400 queries across 6 categories suitable for activation-probe experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZctNve51Iuh2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZctNve51Iuh2",
        "outputId": "0f9c392b-38d1-48de-e37f-0591ca0d3ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /content/queries_400.csv\n",
            "Final count: 400\n",
            "Per-category counts:\n",
            "category\n",
            "fictional_scenarios    351\n",
            "future_events           18\n",
            "out_of_distribution      9\n",
            "obscure_facts            8\n",
            "control                  8\n",
            "knowledge_gaps           6\n",
            "Name: count, dtype: int64\n",
            "Saved human verify sample: /content/human_verify_sample.csv\n",
            "Human sample size: 150\n",
            "\n",
            "================================================================================\n",
            "‚úÖ DATASET GENERATION COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Generated files:\n",
            "  1. queries_400.csv (400 queries)\n",
            "  2. human_verify_sample.csv (150 queries for manual verification)\n",
            "\n",
            "üìã NEXT: Manual Verification\n",
            "  Open human_verify_sample.csv and fill 'verified_label' column:\n",
            "    0 = SAFE (won't hallucinate)\n",
            "    1 = HALLUCINATION (will hallucinate)\n",
            "    0.5 = UNSURE\n",
            "\n",
            "  Then save as: human_verify_sample_VERIFIED.csv\n",
            "================================================================================\n",
            "\n",
            "‚ö†Ô∏è IMPORTANT NOTES:\n",
            "  ‚Ä¢ Category is included for reference only\n",
            "  ‚Ä¢ NEVER use category feature during classifier training\n",
            "  ‚Ä¢ hallucination_likelihood is unverified (category priors)\n",
            "  ‚Ä¢ Use verified labels after you manually check the 150 samples\n",
            "  ‚Ä¢ template_id enables proper cross-validation\n"
          ]
        }
      ],
      "source": [
        "# Run this in Colab / local. Installs dependencies if needed.\n",
        "# pip install -q sentence-transformers scikit-learn pandas tqdm\n",
        "\n",
        "import random, os, time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------- CONFIG --------\n",
        "RANDOM_SEED = 42\n",
        "OUT_DIR = \"/content\"                # change if needed\n",
        "TARGET_TOTAL = 400\n",
        "HUMAN_VERIFY_N = 150\n",
        "NEAR_DUP_THRESHOLD = 0.89           # slightly looser to avoid over-collapse\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "KMEANS_MAX_CLUSTERS = 60\n",
        "MAX_GENERATION_ATTEMPTS = 6         # safety loop to expand until target reached\n",
        "# ------------------------\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Balanced recommended distribution (Option A)\n",
        "CATEGORY_COUNTS = {\n",
        "    \"future_events\": 70,\n",
        "    \"obscure_facts\": 70,\n",
        "    \"fictional_scenarios\": 70,\n",
        "    \"knowledge_gaps\": 70,\n",
        "    \"out_of_distribution\": 70,\n",
        "    \"control\": 50\n",
        "}\n",
        "\n",
        "# Base short template pools (we will programmatically expand these)\n",
        "BASE_TEMPLATES = {\n",
        "    \"future_events\": [\n",
        "        \"Who won the {year} US Presidential Election?\",\n",
        "        \"Describe the {year} Summer Olympics in {city}.\",\n",
        "        \"What was the stock price of Apple on Dec 31, {year}?\",\n",
        "        \"Who is the Prime Minister of India in {year}?\",\n",
        "        \"What is the GDP of India in {year}?\"\n",
        "    ],\n",
        "    \"obscure_facts\": [\n",
        "        \"Summarize the paper 'On Emergent Consciousness in LLMs' by {name}.\",\n",
        "        \"What is the Spectral-Delta Consensus Framework proposed in 2023?\",\n",
        "        \"Describe the research of Professor {name} on quantum cognition.\",\n",
        "        \"What are the main findings of the paper 'Neural Topology and Consciousness'?\"\n",
        "    ],\n",
        "    \"fictional_scenarios\": [\n",
        "        \"What is the capital city of the fictional country {country}?\",\n",
        "        \"Describe the government of the fictional nation {country}.\",\n",
        "        \"Tell me about the fictional island of {place}.\",\n",
        "        \"What is the currency of the fictional country {country}?\"\n",
        "    ],\n",
        "    \"knowledge_gaps\": [\n",
        "        \"Explain the Koebe quarter theorem and its applications.\",\n",
        "        \"Describe the Gromov-Witten invariants in algebraic geometry.\",\n",
        "        \"What is the Sato-Tate conjecture?\",\n",
        "        \"Explain the Yang-Mills existence and mass gap problem.\"\n",
        "    ],\n",
        "    \"out_of_distribution\": [\n",
        "        \"How many colors are in the sound of gravity?\",\n",
        "        \"What does Tuesday taste like combined with the number 7?\",\n",
        "        \"Describe the smell of a mathematical equation.\",\n",
        "        \"If clouds had emotions, what would their favorite song be?\"\n",
        "    ],\n",
        "    \"control\": [\n",
        "        \"What is the capital of France?\",\n",
        "        \"Who is the current Prime Minister of India?\",\n",
        "        \"What year did World War II end?\",\n",
        "        \"What is the chemical formula for water?\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# placeholders pools\n",
        "YEARS = [2026, 2027, 2028, 2029, 2030]\n",
        "CITIES = [\"Paris\", \"Tokyo\", \"Los Angeles\", \"Barcelona\", \"Mumbai\"]\n",
        "NAMES = [\"Fictitious Smith\", \"Dr. Jane Doe\", \"Prof. Made Up\", \"Dr. Unknown\"]\n",
        "COUNTRIES = [\"Norvalis\", \"Atlantica\", \"Zenithia\", \"Eldoria\", \"Mystaria\", \"Aethermoor\"]\n",
        "PLACES = [\"Lemuria\", \"Lumina\", \"Pangoria\", \"Zephyron\", \"Valoria\"]\n",
        "\n",
        "# small paraphrase fragments to expand templates\n",
        "PREFIXES = [\"Could you tell me\", \"Please explain\", \"Do you know\", \"Give an outline of\", \"\"]\n",
        "SUFFIXES = [\"in brief\", \"in detail\", \"if available\", \"‚Äî be concise\", \"please\"]\n",
        "\n",
        "def safe_format(template):\n",
        "    params = {}\n",
        "    if \"{year}\" in template:\n",
        "        params[\"year\"] = random.choice(YEARS)\n",
        "    if \"{city}\" in template:\n",
        "        params[\"city\"] = random.choice(CITIES)\n",
        "    if \"{name}\" in template:\n",
        "        params[\"name\"] = random.choice(NAMES)\n",
        "    if \"{country}\" in template:\n",
        "        params[\"country\"] = random.choice(COUNTRIES)\n",
        "    if \"{place}\" in template:\n",
        "        params[\"place\"] = random.choice(PLACES)\n",
        "    try:\n",
        "        return template.format(**params)\n",
        "    except Exception:\n",
        "        return template\n",
        "\n",
        "def paraphrase_variant(s):\n",
        "    # light paraphrase: add prefix/suffix, change phrasing, optional clause\n",
        "    p = s\n",
        "    if random.random() < 0.35:\n",
        "        pre = random.choice(PREFIXES)\n",
        "        if pre:\n",
        "            p = pre + \" \" + p[0].lower() + p[1:] if p[0].isupper() else pre + \" \" + p\n",
        "    if random.random() < 0.35:\n",
        "        suf = random.choice(SUFFIXES)\n",
        "        if suf:\n",
        "            p = p.rstrip(\".?\") + f\" ({suf}).\"\n",
        "    # small structural rewrite\n",
        "    if random.random() < 0.12:\n",
        "        p = p.replace(\"Describe the\", \"Give an overview of the\")\n",
        "    if random.random() < 0.10:\n",
        "        p = p.replace(\"What is\", \"Could you explain what is\")\n",
        "    if random.random() < 0.08:\n",
        "        p = p + \" Please be precise.\"\n",
        "    return p\n",
        "\n",
        "def expand_templates_for_category(base_list, needed):\n",
        "    \"\"\"Create a varied pool from a small base list by combining paraphrase and minor rewrites.\"\"\"\n",
        "    pool = set()\n",
        "    attempts = 0\n",
        "    while len(pool) < needed and attempts < needed * 10:\n",
        "        t = random.choice(base_list)\n",
        "        filled = safe_format(t)\n",
        "        # randomly combine two short templates to create hybrid harder queries (for knowledge gaps)\n",
        "        if random.random() < 0.05 and len(base_list) > 1:\n",
        "            other = safe_format(random.choice(base_list))\n",
        "            filled = f\"{filled} Also, {other[0].lower() + other[1:]}\"\n",
        "        variant = paraphrase_variant(filled)\n",
        "        pool.add(variant.strip())\n",
        "        attempts += 1\n",
        "    return list(pool)\n",
        "\n",
        "def generate_raw_queries(target_map, multiplier=1.5):\n",
        "    all_rows = []\n",
        "    for cat, target in target_map.items():\n",
        "        raw_needed = int(target * multiplier)\n",
        "        base = BASE_TEMPLATES[cat]\n",
        "        expanded = expand_templates_for_category(base, max(raw_needed, len(base)*10))\n",
        "        # pick raw_needed samples (allow repeats with paraphrase)\n",
        "        picks = []\n",
        "        while len(picks) < raw_needed:\n",
        "            picks.append(random.choice(expanded))\n",
        "        # assign base hallucination likelihood (kept for traceability, not used for training directly)\n",
        "        base_likelihood = 0.95 if cat in (\"future_events\",\"obscure_facts\",\"fictional_scenarios\") else 0.85 if cat==\"knowledge_gaps\" else 0.99 if cat==\"out_of_distribution\" else 0.05\n",
        "        for q in picks:\n",
        "            all_rows.append({\n",
        "                \"query_text\": q,\n",
        "                \"category\": cat,\n",
        "                \"hallucination_likelihood\": base_likelihood\n",
        "            })\n",
        "    return pd.DataFrame(all_rows)\n",
        "\n",
        "# Main generation loop: generate more than needed then collapse near-duplicates\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "attempt = 0\n",
        "final_df = None\n",
        "while attempt < MAX_GENERATION_ATTEMPTS:\n",
        "    attempt += 1\n",
        "    raw_df = generate_raw_queries(CATEGORY_COUNTS, multiplier=1.6)  # produce extra variants\n",
        "    raw_df = raw_df.sample(frac=1, random_state=RANDOM_SEED+attempt).reset_index(drop=True)  # shuffle\n",
        "    # drop exact duplicates\n",
        "    raw_df = raw_df.drop_duplicates(subset=\"query_text\").reset_index(drop=True)\n",
        "    texts = raw_df['query_text'].astype(str).tolist()\n",
        "    embs = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "    # pairwise cosine sim\n",
        "    sim = cosine_similarity(embs)\n",
        "    to_drop = set()\n",
        "    n = len(raw_df)\n",
        "    for i in range(n):\n",
        "        if i in to_drop:\n",
        "            continue\n",
        "        for j in range(i+1, n):\n",
        "            if j in to_drop:\n",
        "                continue\n",
        "            if sim[i,j] > NEAR_DUP_THRESHOLD:\n",
        "                to_drop.add(j)\n",
        "    df_clean = raw_df.drop(index=list(to_drop)).reset_index(drop=True)\n",
        "    # if too small, lower threshold slightly or regenerate variants\n",
        "    if len(df_clean) >= TARGET_TOTAL:\n",
        "        final_df = df_clean.copy()\n",
        "        break\n",
        "    else:\n",
        "        # loosen threshold and try again (but only moderate change)\n",
        "        NEAR_DUP_THRESHOLD -= 0.01\n",
        "        if NEAR_DUP_THRESHOLD < 0.82:\n",
        "            # increase multiplier for raw generation instead\n",
        "            # this path should rarely happen\n",
        "            final_df = df_clean.copy()\n",
        "            break\n",
        "\n",
        "# Final safety: if still under target, expand by paraphrasing remaining templates\n",
        "if final_df is None:\n",
        "    final_df = df_clean.copy()\n",
        "\n",
        "if len(final_df) < TARGET_TOTAL:\n",
        "    needed = TARGET_TOTAL - len(final_df)\n",
        "    # create additional paraphrases from existing rows\n",
        "    additions = []\n",
        "    source_texts = final_df['query_text'].tolist() if len(final_df)>0 else raw_df['query_text'].tolist()\n",
        "    idx = 0\n",
        "    while len(additions) < needed:\n",
        "        s = source_texts[idx % len(source_texts)]\n",
        "        pv = paraphrase_variant(s + \" Please elaborate.\")\n",
        "        additions.append({\"query_text\": pv, \"category\": final_df['category'].mode()[0] if 'category' in final_df.columns else \"control\", \"hallucination_likelihood\": 0.5})\n",
        "        idx += 1\n",
        "    extra_df = pd.DataFrame(additions)\n",
        "    final_df = pd.concat([final_df, extra_df], ignore_index=True)\n",
        "\n",
        "# Recompute embeddings and cluster to produce template_id\n",
        "texts_final = final_df['query_text'].astype(str).tolist()\n",
        "embs_final = embedder.encode(texts_final, convert_to_numpy=True, show_progress_bar=False)\n",
        "k = min(KMEANS_MAX_CLUSTERS, max(2, len(final_df)//5))\n",
        "kmeans = KMeans(n_clusters=k, random_state=RANDOM_SEED, n_init=10)\n",
        "labels = kmeans.fit_predict(embs_final)\n",
        "final_df['template_id'] = labels\n",
        "\n",
        "# Ensure stable IDs and minimal schema\n",
        "final_df = final_df.reset_index(drop=True)\n",
        "final_df['query_id'] = final_df.index.astype(int)\n",
        "final_df = final_df[['query_id', 'query_text', 'category', 'hallucination_likelihood', 'template_id']]\n",
        "\n",
        "# If we overshot slightly, trim to TARGET_TOTAL (keep stratified per category)\n",
        "if len(final_df) > TARGET_TOTAL:\n",
        "    # keep proportional per-category\n",
        "    keep_df = []\n",
        "    for cat, cnt in CATEGORY_COUNTS.items():\n",
        "        cat_rows = final_df[final_df['category']==cat]\n",
        "        keep_n = int(cnt)\n",
        "        # if not enough in cat, take all\n",
        "        keep_df.append(cat_rows.sample(n=min(keep_n, len(cat_rows)), random_state=RANDOM_SEED))\n",
        "    final_df = pd.concat(keep_df).drop_duplicates().reset_index(drop=True)\n",
        "    # if still < TARGET_TOTAL, pad randomly\n",
        "    if len(final_df) < TARGET_TOTAL:\n",
        "        pad_needed = TARGET_TOTAL - len(final_df)\n",
        "        candidates = final_df.sample(n=pad_needed, replace=True, random_state=RANDOM_SEED)\n",
        "        final_df = pd.concat([final_df, candidates]).reset_index(drop=True)\n",
        "\n",
        "# Save CSV\n",
        "out_path = os.path.join(OUT_DIR, \"queries_400.csv\")\n",
        "final_df.to_csv(out_path, index=False)\n",
        "print(\"Saved:\", out_path)\n",
        "print(\"Final count:\", len(final_df))\n",
        "print(\"Per-category counts:\")\n",
        "print(final_df['category'].value_counts())\n",
        "\n",
        "# Human-verify sample (balanced)\n",
        "cats = final_df['category'].unique().tolist()\n",
        "per_cat = max(1, HUMAN_VERIFY_N // len(cats))\n",
        "hv = []\n",
        "for c in cats:\n",
        "    sub = final_df[final_df['category']==c]\n",
        "    n = min(per_cat, len(sub))\n",
        "    hv.append(sub.sample(n=n, random_state=RANDOM_SEED))\n",
        "hv = pd.concat(hv).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "if len(hv) < HUMAN_VERIFY_N:\n",
        "    remaining = HUMAN_VERIFY_N - len(hv)\n",
        "    extra = final_df.drop(hv.index).sample(n=remaining, random_state=RANDOM_SEED)\n",
        "    hv = pd.concat([hv, extra]).reset_index(drop=True)\n",
        "hv_path = os.path.join(OUT_DIR, \"human_verify_sample.csv\")\n",
        "hv.to_csv(hv_path, index=False)\n",
        "print(\"Saved human verify sample:\", hv_path)\n",
        "print(\"Human sample size:\", len(hv))\n",
        "\n",
        "hv['verified_label'] = None\n",
        "hv_path = os.path.join(OUT_DIR, \"human_verify_sample.csv\")\n",
        "hv.to_csv(hv_path, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ DATASET GENERATION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nGenerated files:\")\n",
        "print(f\"  1. queries_400.csv ({len(final_df)} queries)\")\n",
        "print(f\"  2. human_verify_sample.csv ({len(hv)} queries for manual verification)\")\n",
        "print(f\"\\nüìã NEXT: Manual Verification\")\n",
        "print(f\"  Open human_verify_sample.csv and fill 'verified_label' column:\")\n",
        "print(f\"    0 = SAFE (won't hallucinate)\")\n",
        "print(f\"    1 = HALLUCINATION (will hallucinate)\")\n",
        "print(f\"    0.5 = UNSURE\")\n",
        "print(f\"\\n  Then save as: human_verify_sample_VERIFIED.csv\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è IMPORTANT NOTES:\")\n",
        "print(\"  ‚Ä¢ Category is included for reference only\")\n",
        "print(\"  ‚Ä¢ NEVER use category feature during classifier training\")\n",
        "print(\"  ‚Ä¢ hallucination_likelihood is unverified (category priors)\")\n",
        "print(\"  ‚Ä¢ Use verified labels after you manually check the 150 samples\")\n",
        "print(\"  ‚Ä¢ template_id enables proper cross-validation\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cca0859",
      "metadata": {
        "id": "0cca0859"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4zB6j5VlSOG3",
      "metadata": {
        "id": "4zB6j5VlSOG3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
