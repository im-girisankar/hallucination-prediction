{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13773718,"sourceType":"datasetVersion","datasetId":8766445}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================================\n# Mistral 7B v0.3 Activation Extraction (Kaggle Version)\n# ============================================================================\n\n# -------- INSTALL --------\n!pip install -q -U transformers accelerate bitsandbytes huggingface_hub\n\n# -------- IMPORTS --------\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport torch\nimport shutil\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# -------- CONFIG --------\n# CHANGED: Updated to Mistral v0.3\nMODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n\n# Auto-detect input CSV path\ninput_files = glob.glob(\"/kaggle/input/**/queries_400.csv\", recursive=True)\nif input_files:\n    QUERIES_PATH = input_files[0]\n    print(f\"âœ… Found input file: {QUERIES_PATH}\")\nelse:\n    raise FileNotFoundError(\"âŒ Could not find 'queries_400.csv' in /kaggle/input.\")\n\nOUTPUT_DIR = \"/kaggle/working/ACTIVATIONS\"\n# Mistral has 32 layers. Layers 3,4,5,6 are valid low-level features.\nLAYER_INDICES = [3, 4, 5, 6] \nMAX_LEN = 1024\nFWD_BATCH = 4\nSAVE_BATCH = 40\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# -------- KAGGLE SECRETS AUTH --------\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n    login(token=hf_token)\n    print(\"âœ… Logged into HF via Kaggle Secrets\")\nexcept Exception as e:\n    print(f\"âŒ Auth failed. Check 'HF_TOKEN' in Add-ons -> Secrets. Error: {e}\")\n\n# -------- LOAD MODEL (4-BIT QUANTIZED) --------\nprint(f\"ðŸš€ Loading {MODEL_ID} with 4-bit quantization...\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_token)\n\n# Mistral often lacks a default pad token, setting it to EOS is standard practice\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    token=hf_token,\n    device_map=\"auto\",\n    output_hidden_states=True,\n    quantization_config=quant_config,\n    trust_remote_code=True\n)\nmodel.eval()\nprint(f\"âœ… Model loaded: {next(model.parameters()).device}\")\n\n# -------- LOAD QUERIES --------\ndf = pd.read_csv(QUERIES_PATH)\nif 'query_id' not in df.columns:\n    df['query_id'] = range(len(df))\ndf = df.sort_values('query_id').reset_index(drop=True)\nqueries = df['query_text'].astype(str).tolist()\nprint(f\"âœ… Loaded {len(queries)} queries\")\n\n# -------- VALIDATE LAYERS --------\nnum_layers = model.config.num_hidden_layers\nvalid_layers = [li for li in LAYER_INDICES if li + 1 < num_layers]\nprint(f\"âœ… Valid layers: {valid_layers}\")\n\n# -------- EXTRACTION --------\n@torch.inference_mode()\ndef extract_hidden_batch(text_list):\n    enc = tokenizer(text_list, return_tensors=\"pt\", padding=True, \n                    truncation=True, max_length=MAX_LEN).to(model.device)\n    out = model(**enc, output_hidden_states=True, return_dict=True)\n    hidden = out.hidden_states\n    \n    pooled_layers = []\n    for li in valid_layers:\n        # Mistral structure is similar to Llama: hidden[0] is embeddings\n        layer_tensor = hidden[li + 1] \n        pooled = layer_tensor.mean(dim=1)\n        pooled_layers.append(pooled.cpu().numpy())\n    \n    return np.stack(pooled_layers, axis=1).astype(np.float32)\n\n# -------- MAIN LOOP --------\nprint(f\"\\nðŸŽ¯ Extracting activations for Mistral...\")\nmeta_rows = []\nbatch_idx = 0\nsafe_model_name = MODEL_ID.replace(\"/\", \"_\")\n\nfor start_idx in range(0, len(queries), SAVE_BATCH):\n    end_idx = min(len(queries), start_idx + SAVE_BATCH)\n    batch_queries = queries[start_idx:end_idx]\n    \n    sub_outputs = []\n    for i in range(0, len(batch_queries), FWD_BATCH):\n        sub_texts = batch_queries[i:i+FWD_BATCH]\n        try:\n            acts = extract_hidden_batch(sub_texts)\n            sub_outputs.append(acts)\n        except RuntimeError as e:\n            if \"out of memory\" in str(e).lower():\n                print(f\"  OOM, fallback to single...\")\n                for single in sub_texts:\n                    sub_outputs.append(extract_hidden_batch([single]))\n            else:\n                raise\n    \n    batch_acts = np.vstack(sub_outputs)\n    \n    # Save\n    npz_name = f\"{safe_model_name}_acts_{batch_idx:03d}_{start_idx}_{end_idx-1}.npz\"\n    npz_path = os.path.join(OUTPUT_DIR, npz_name)\n    np.savez_compressed(\n        npz_path,\n        activations=batch_acts,\n        query_ids=df.iloc[start_idx:end_idx]['query_id'].to_numpy(),\n        categories=df.iloc[start_idx:end_idx].get('category', ['unknown']*(end_idx-start_idx)).to_numpy()\n    )\n    print(f\"  âœ… Batch {batch_idx}: {npz_name} ({os.path.getsize(npz_path)/1e6:.1f}MB)\")\n    \n    # Metadata\n    for local_idx, qid in enumerate(df.iloc[start_idx:end_idx]['query_id'].tolist()):\n        meta_rows.append({\n            \"query_id\": int(qid),\n            \"npz_file\": npz_name,\n            \"local_index\": int(local_idx),\n            \"category\": df.iloc[start_idx + local_idx].get('category', 'unknown')\n        })\n    \n    batch_idx += 1\n\n# -------- SAVE INDEX & ZIP --------\nmeta_df = pd.DataFrame(meta_rows)\nindex_csv = os.path.join(OUTPUT_DIR, f\"{safe_model_name}_activation_index.csv\")\nmeta_df.to_csv(index_csv, index=False)\n\nprint(f\"\\nâœ… COMPLETE: Output saved to {OUTPUT_DIR}\")\n\nprint(\"ðŸ“¦ Zipping output for download...\")\nshutil.make_archive(\"/kaggle/working/mistral_activations_output\", 'zip', OUTPUT_DIR)\nprint(\"âœ… Ready! Download 'mistral_activations_output.zip' from the Output tab.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T07:16:50.890227Z","iopub.execute_input":"2025-11-18T07:16:50.890888Z","iopub.status.idle":"2025-11-18T07:19:02.037056Z","shell.execute_reply.started":"2025-11-18T07:16:50.890863Z","shell.execute_reply":"2025-11-18T07:19:02.036392Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"âœ… Found input file: /kaggle/input/hallucination-detection-dataset-400/queries_400.csv\nâœ… Logged into HF via Kaggle Secrets\nðŸš€ Loading mistralai/Mistral-7B-Instruct-v0.3 with 4-bit quantization...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"732cd72482bf419fb9a861c00c0ea973"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8ef102b4e4649e5a40838bd519a1bef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f9f38a6606048d199b103fa16a06857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23c0418f218497091d051d1f727d985"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6c041a1e724464391455850df3046e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4420250c8e3045579c39ed6f190b05f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a8d39b482844c238f6b7ee40dc6abd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c71eb6dad6534126b19235c362a160d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0455074140ad4141863739c676c942ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6a4aa5fc3ae42e0b0ff13153229d3fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dfdac5649ee4e7e912d1a364752b212"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9df9aab4bb254936b18d50cd5676bcd6"}},"metadata":{}},{"name":"stdout","text":"âœ… Model loaded: cuda:0\nâœ… Loaded 400 queries\nâœ… Valid layers: [3, 4, 5, 6]\n\nðŸŽ¯ Extracting activations for Mistral...\n  âœ… Batch 0: mistralai_Mistral-7B-Instruct-v0.3_acts_000_0_39.npz (1.6MB)\n  âœ… Batch 1: mistralai_Mistral-7B-Instruct-v0.3_acts_001_40_79.npz (1.6MB)\n  âœ… Batch 2: mistralai_Mistral-7B-Instruct-v0.3_acts_002_80_119.npz (1.6MB)\n  âœ… Batch 3: mistralai_Mistral-7B-Instruct-v0.3_acts_003_120_159.npz (1.6MB)\n  âœ… Batch 4: mistralai_Mistral-7B-Instruct-v0.3_acts_004_160_199.npz (1.6MB)\n  âœ… Batch 5: mistralai_Mistral-7B-Instruct-v0.3_acts_005_200_239.npz (1.6MB)\n  âœ… Batch 6: mistralai_Mistral-7B-Instruct-v0.3_acts_006_240_279.npz (1.6MB)\n  âœ… Batch 7: mistralai_Mistral-7B-Instruct-v0.3_acts_007_280_319.npz (1.6MB)\n  âœ… Batch 8: mistralai_Mistral-7B-Instruct-v0.3_acts_008_320_359.npz (1.6MB)\n  âœ… Batch 9: mistralai_Mistral-7B-Instruct-v0.3_acts_009_360_399.npz (1.6MB)\n\nâœ… COMPLETE: Output saved to /kaggle/working/ACTIVATIONS\nðŸ“¦ Zipping output for download...\nâœ… Ready! Download 'mistral_activations_output.zip' from the Output tab.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}